{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model for ASL Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Data And Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get all the data, I will loop through all the videos for all the words, and get the numpy arrays of each frame I saved during the pre-processing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 3\n",
    "num_videos = 100\n",
    "start_frames = 5\n",
    "end_frames = 75\n",
    "videos_base_dir = 'video_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, I defined to start getting the frames starting from the 5th frame. This is because I found out that the cv2 module keeps recording videos even when I put a waitKey(2000) to wait 2 seconds to get into position for each video. I was not able to find any solutions or reasons why this is happening. So, I will instead just disregard the first 5 frames of each video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "for words in range(num_words):\n",
    "    for videos in range(num_videos):\n",
    "        video_array_list = []\n",
    "        for frames in range(start_frames, end_frames):\n",
    "            numpy_array_file_path = os.path.join(videos_base_dir,f'{words}',f'{videos}', 'landmarks', f'{words}_{videos}_{frames}.npy')\n",
    "            temp_array = np.load(numpy_array_file_path)\n",
    "            #print(f'Loading {numpy_array_file_path}')\n",
    "            video_array_list.append(temp_array)\n",
    "        data.append(video_array_list)\n",
    "        labels.append(words)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if this worked, I will print the shape of the data and labels and also look at some of the data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data: (300, 70, 1662)\n",
      "Shape of labels: (300,)\n"
     ]
    }
   ],
   "source": [
    "data_np = np.array(data)\n",
    "print(f'Shape of data: {data_np.shape}')\n",
    "\n",
    "labels_np = np.array(labels)\n",
    "print(f'Shape of labels: {labels_np.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape seems right. I will do some quick checks by grabbing random frames and checking with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in range(num_words):\n",
    "    for video in range(num_videos):\n",
    "        for frame in range(start_frames, end_frames):\n",
    "            if np.sum(data_np[video + (word*100)][frame-5] != np.load(os.path.join(videos_base_dir,f'{word}',f'{video}', 'landmarks', f'{word}_{video}_{frame}.npy'))):\n",
    "                print(f'Error at {word}, {video}, {frame}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this means all of the data was loaded successfully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomly Splitting The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is randomizing this data and then splitting it into 3 sections. Training data, Validation data, and Testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(113)\n",
    "data_indices = np.arange(len(data))\n",
    "#print(data_indices)\n",
    "rng.shuffle(data_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[151  79 268 198 177   6  92 269  34  12 233 144 175 259  32  33  40 231\n",
      " 255  96 132  21  71 181  19  22 152 146  35 115 142 154 156 157 148 117\n",
      " 272 133  62 141   3  56 143  63 221 147 182 210 145 260  46 298 294 257\n",
      " 174  43 124  29 126 171  16  25  80  30  41  91 106  26 114 108  93  20\n",
      " 158 120   0 119 212 219 134 299  44 168  82 229 234   5 105 286 129 247\n",
      " 285 297 125  17  68 178  81 137 155 275   9 203  86  39  28 209 238 187\n",
      " 251 192 250  94 195   8  78 283  24 236 256  42 131 226 169 123 293 190\n",
      " 227 196 204 109  14 262  59 202  50  57 225  66 176 241 217   2 113  18\n",
      " 200 159 166 264 103  13  51  84  58 107 237 140   4 121  48 104 111 205\n",
      "  60 271 163 161  45 265  27 191 127 180  77  65 248 201  72  37 295  61\n",
      " 110 130  98  53  99 223 214  75 232  10  54 164 128 179  90 292  52 188\n",
      " 270 136 135 197 289  38 224 101  89 138 242 208 207 243 211 170 273 100\n",
      " 239 263 206 118  49  15  76 216 153 194 291 112 235 172 160  87 290 193\n",
      "  88  83 122 184 278 246  31 139 186  70 296 287 102 280 249  36 245 282\n",
      " 199 222  97 173  67 116  73 277 149 167  64  95 266 228 183 185 276 215\n",
      "  55  23 189  85 267 274   1 254 284 252 288  11  69 213 281 230 261 244\n",
      "   7 218  47 240 253 279 150 258  74 162 220 165]\n"
     ]
    }
   ],
   "source": [
    "print(data_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I have a randomized indices array. I can split this array into 3, and then get the data at these indices for each section. The split will be 70% training, 15% validation, and 15% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = data_indices[0:210]\n",
    "val_indices = data_indices[210:255]\n",
    "test_indices = data_indices[255:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_np[train_indices]\n",
    "y_train = labels_np[train_indices]\n",
    "\n",
    "x_val = data_np[val_indices]\n",
    "y_val = labels_np[val_indices]\n",
    "\n",
    "x_test = data_np[test_indices]\n",
    "y_test = labels_np[test_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will do some quick manual checks by looking at the randomized indices that were generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(x_train[5][5] == np.load('video_data/0/6/landmarks/0_6_10.npy'))\n",
    "print(y_train[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(x_val[41][15] == np.load('video_data/2/82/landmarks/2_82_20.npy'))\n",
    "print(y_val[41])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True ...  True  True  True]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(x_test[44][50] == np.load('video_data/1/65/landmarks/1_65_55.npy'))\n",
    "print(y_test[44])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok it seems like the random shuffling has worked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating And Training The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have training, testing, and validation data, I will create a model, compile it, and then train it using this data. Then I will run some predictions using the testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating A Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece5831-2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
